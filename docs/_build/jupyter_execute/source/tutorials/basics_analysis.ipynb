{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2174f28",
   "metadata": {},
   "source": [
    "# Getting Started: Network Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1086945e",
   "metadata": {},
   "source": [
    "The network analysis code estimates the Expected Information Gain (EIG) of a given seismic monitoring networks for a given prior distribution of potential events. The code samples these candidate events and then generates synthetic datasets that could plausibly be seen by the sensors. For each of the datasets, the code constructs the posterior distribution and computes the information gain (IG) according to the KL-divergence. This information gain is averaged overall synthetic datasets to compute the EIG. The code can also return a list of the IG for different hypothetical events which can be used to generate a map of sensitivities of the network to different event locations, depths, and magnitudes.\n",
    "\n",
    "The network analysis code is contained in the python script `eig_calc.py`. This script takes three arguments: a configuration file, an output file location, and a verbosity control. An example configuration file, which we call `inputs.dat`, might look like this:\n",
    "\n",
    "```text  \n",
    "128  \n",
    "256  \n",
    "2  \n",
    "ta_array_domain.json  \n",
    "uniform_prior.py  \n",
    "40.0, -111.5,0.1,2,0  \n",
    "41.0, -111.9,0.1,2,0  \n",
    "42.0, -109.0,0.1,2,0  \n",
    "```\n",
    "\n",
    "The first line specifies how many events will be used to generate the synthetic data that will be used to compute $KL\\left[p(\\theta | D) || p(\\theta)\\right].$\n",
    "The second line specifies how many events will be used to discretize the event domain when computing\n",
    "$KL[p(\\theta | D) p(\\theta)].$\n",
    "Line 3 specifies how many realizations of data to generate for each event. Line 4 is the path to the file containing the bounds on the events to be generated, and Line 5 is the path to the file containing the functions for sampling events. For more information on input files, For more information on input files, see [Writing input files](inputs.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7851527d",
   "metadata": {},
   "source": [
    "(run-the-code)=\n",
    "## Running the code\n",
    "The code can be run interactively, either locally or on an HPC system, or it can be run through a HPC scheduler. This tutorial assumes the HPC system uses [Slurm](https://slurm.schedmd.com/documentation.html).\n",
    "\n",
    "The code for analyzing hte network is contained in the Python file `eig_calc.py`, which is executed with the with the following arguments:\n",
    "\n",
    "-   Input file: path to the input file (see [Writing input files](inputs.ipynb) for more details)\n",
    "\n",
    "-   Output Numpy file: Path to the location and filename where the\n",
    "    outputs will be saved. File must be in `.npz` format.\n",
    "\n",
    "-   Verbosity: One of 3 verbosity levels may be specified: `0`, `1`, or\n",
    "    `2`:\n",
    "\n",
    "    -   `0` has no output other than the final EIG estimate, EIG\n",
    "        Standard Deviation, and Statistic of EIG effective sample size.\n",
    "\n",
    "    -   `1` is the most verbose with printed statements throughout the\n",
    "        code describing what is going on and all the different\n",
    "        calculated quantities are stored in the output file.\n",
    "\n",
    "    -   `2` is a more limited version of 1 with less IO and quantities\n",
    "        stored in the output file. See [Code outputs](code_outputs) for a list of which quantities are\n",
    "        stored in the output file for each verbosity level.\n",
    "  \n",
    "### Running locally\n",
    "To run the code on a local machine, execute it with `mpi` like so:\n",
    "```shell\n",
    "mpiexec -n 4 python3 eig_calc.py inputs.dat outputs.npz 1\n",
    "```\n",
    "The flag `-n` specifies how many cores `mpiexec` should use to execute `eig_calc.py`.\n",
    "\n",
    "### Running using HPC interactively\n",
    "To submit an interactive job, use the `salloc` command. The command\n",
    "`salloc` requests a slurm allocation, and has several flags that are\n",
    "used to specify the details of the allocation. This varies by system, \n",
    "but typically the number of nodes and the allocation time are required:\n",
    "\n",
    "-   `-nodes`: The number of nodes to request.\n",
    "\n",
    "-   `--time`: The time the nodes will be allocated to your account\n",
    "\n",
    "An example job allocation request looks like this:\n",
    "\n",
    "```shell\n",
    "salloc --nodes=2 --time=2:00:00\n",
    "```\n",
    "\n",
    "This command is requesting 2\n",
    "nodes for a length of 2 hours. For more details on `salloc`, see the\n",
    "Slurm documentation: <https://slurm.schedmd.com/documentation.html>.\n",
    "\n",
    "Once you have an allocation, you can now submit the job. This is done\n",
    "with the `mpiexec` command which requires the following arguments:\n",
    "\n",
    "-   `--bind-to-core` : Binds each process to a physical core\n",
    "\n",
    "-   `--npernode` : Specifies the number of cores per node\n",
    "\n",
    "-   `--n` : Specifies the total number of available cores\n",
    "\n",
    "-   Executable to be run with mpi along with all the arguments to the\n",
    "    executable.\n",
    "\n",
    "In our case, the executable is `eig_calc.py` with its subsequent arguments as defined above.\n",
    "\n",
    "For example, to run a job on a machine that has 16 cores per node, we\n",
    "could use the following:\n",
    "```shell\n",
    "mpiexec --bind-to core --npernode 16 --n 32 python3 eig_calc.py inputs.dat outputs.npz 1\n",
    "```\n",
    "which submits a job using 2 nodes (16 cores per node times 2 nodes gives\n",
    "32 total cores), reads the input data from `inputs.dat`, saves the\n",
    "output data to `outputs.npz`, and uses verbose setting 1.\n",
    "\n",
    "### Running on HPC with script \n",
    "\n",
    "A bash script can be written that will submit a job to the HPC job\n",
    "queue. This does not require the user to specifically allocate nodes to\n",
    "use for the job; nodes will be allocated and the job will begin\n",
    "automatically once the number of nodes specified in the bash script are\n",
    "available.\n",
    "An example script might look like\n",
    "```text\n",
    "#!/bin/bash\n",
    "## Do not put any commands or blank lines before the #SBATCH lines\n",
    "#SBATCH --nodes=16                   # Number of nodes - all cores \n",
    "                                     #per node are allocated to the job\n",
    "#SBATCH --time=04:00:00              # Wall clock time (HH:MM:SS) - \n",
    "                                     # once the job exceeds this time, the job will be terminated (default is 5 minutes)\n",
    "#SBATCH --job-name=seis_eig          # Name of job\n",
    "#SBATCH --export=ALL                 # export environment variables from the submission env\n",
    "                                     # (modules, bashrc etc)\n",
    "\n",
    "nodes=$SLURM_JOB_NUM_NODES           # Number of nodes - the number of \n",
    "                                     # nodes you have requested (for a \n",
    "                                     # list of SLURM environment \n",
    "                                     # variables see \"man sbatch\")\n",
    "cores=16                             # Number MPI processes to run \n",
    "                                     # on each node (a.k.a. PPN)\n",
    "                                   \n",
    "\n",
    "mpiexec --bind-to core --npernode $cores --n $(($cores*$nodes)) python3 -u eig_calc.py inputs.dat outputs.npz 0\n",
    "```\n",
    "\n",
    "This script can then be submitted\n",
    "using the `sbatch` command:\n",
    "```shell\n",
    "sbatch eig_batch_submission_script.bash\n",
    "```\n",
    "For a comprehensive\n",
    "list of available options, see the Slurm documentation\n",
    "(<https://slurm.schedmd.com/documentation.html>), and in particular the\n",
    "Slurm command/option summary, found\n",
    "[here](https://slurm.schedmd.com/documentation.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfdbcca",
   "metadata": {},
   "source": [
    "(code_outputs)=\n",
    "### Code outputs\n",
    "At the end of running `eig_calc.py`, the code will print to screen the\n",
    "following results:\n",
    "\n",
    "-   Expected information gain (EIG) of the sensor network\n",
    "\n",
    "-   Standard deviation (STD) of the expected information gain\n",
    "\n",
    "-   Minimum effective sample size (ESS) for all realizations of\n",
    "    synthetic data of the weighted samples that make up the posterior\n",
    "    distribution estimate\n",
    "\n",
    "In some sense, the STD and ESS numbers relate to the variance and bias\n",
    "of the EIG estimator. Additionally, an output Numpy file (e.g.\n",
    "`outputs.npz`) is created. The variables stored in this file depend on\n",
    "the verbose flag. The table below provides a full description of the output.\n",
    "\n",
    "| Variable Name | Description                                      | Verbose Levels |\n",
    "| :-------      | :----------------------------                    | :---------------------------------------- |\n",
    "| `eig`         |Estimated expected information gain (EIG) over all experiments | 0, 1, 2 |\n",
    "| `seig`        |Standard deviation of EIG over all experiments                   |0, 1, 2|\n",
    "| `ig`          |Information gain assessed for each experiment   |1, 2|\n",
    "| `ess`         |Effective sample size (ESS) of weighted posterior samples for each experiment |1, 2|\n",
    "| `miness`      |Minimum ESS over all experiments  |1,2|\n",
    "| `theta_data`  |Parameters (latitude, longitude, depth, magnitude) of synthetic events|1, 2|\n",
    "| `theta_space` |Parameters (latitude, longitude, depth, magnitude) that define the posterior event space |1, 2|\n",
    "| `sensors`     |Sensor network configuration as defined in the input file |1, 2|\n",
    "| `lat_range`   |Latitude range of events as defined in the input file |1, 2|\n",
    "| `lon_range`   |Longitude range of events as defined in the input file |1, 2|\n",
    "| `depth_range` |Depth range of events as defined in the input file |1, 2|\n",
    "| `mag_range`   |Magnitude range of events as defined in the input file |1, 2|\n",
    "| `loglikes`    |Log-likelihood of candidate event for every synthetic experiment |1|\n",
    "| `dataz`       |Data used in each synthetic experiment |1|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe40632d",
   "metadata": {},
   "source": [
    "## Examples\n",
    "The prior distribution encodes the experimenter's _a priori_ belief about the quantities of interest $\\theta$ before observing any data. In this case, the quantities of interest describe a seismic source:\n",
    "$$\\theta = [\\verb|latitude, longitude, depth, magnitude|]$$.\n",
    "\n",
    "We assume that each variable is independent, so the prior is\n",
    "\n",
    "$$\n",
    "p(\\theta) = p(\\verb|latitude|)p(\\verb|longitude|)p(\\verb|depth|)p(\\verb|magnitude|).\n",
    "$$\n",
    "\n",
    "\n",
    "In both cases, we analyze a simple\n",
    "network that looks like this:\n",
    "\n",
    "(example-sensor-domain)=\n",
    ":::{figure} ../figs/sensor_network.png\n",
    "---\n",
    "width: 80 %\n",
    "---\n",
    "Example sensor domain\n",
    ":::\n",
    "\n",
    "In each example, we will use 4096 events in a space that's discretized\n",
    "by 16,384 events. We'll create 32 realizations of data for each sensor,\n",
    "and we'll work in a lat/lon domain as shown in the\n",
    "[example sensor domain figure](example-sensor-domain), a\n",
    "depth domain of \\[0,40\\] and a magnitude domain of \\[.5,9.5\\]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc296d4",
   "metadata": {},
   "source": [
    "### Analysis with uniform prior\n",
    "In this example, we assume a uniform distribution on latitude, longitude, and depth, and assume magnitude follows an exponential distribution:\n",
    "\n",
    "$$\n",
    "\\verb|magnitude| \\sim \\text{Exp}(10).\n",
    "$$\n",
    "\n",
    "The functions that perform the sampling operations for $\\theta$ can be found in the file [`uniform_prior.py`](https://github.com/sandialabs/seismic_boed/blob/master/examples/sampling_files/uniform_prior.py).\n",
    "For more information on how to write a sampling file, see #TODO.\n",
    "\n",
    "\n",
    "\n",
    "These sensors are all of type `0`, meaning they produce data based on\n",
    "seismic waves, and they have a uniform SNR offset value of 0. We first\n",
    "need to write an input file according to the guidelines in Section\n",
    "[\\[subsec:input_file\\]](#subsec:input_file){reference-type=\"ref\"\n",
    "reference=\"subsec:input_file\"}. This input file can be the exact same no\n",
    "matter what prior we use, with the only change being in Line 8 where the\n",
    "sampling file is specified. Once this line has been specified correctly,\n",
    "all other steps will be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e05c50",
   "metadata": {},
   "source": [
    "#### Input file\n",
    "The input file we will use looks like this:\n",
    "```text\n",
    "4096\n",
    "16384\n",
    "32\n",
    "uniform_prior.py\n",
    "40.5,-109,0,2.,0.\n",
    "40.5,-110,0,2.,0.\n",
    "40.5,-111,0,2.,0.\n",
    "41,-109,0,2.,0.\n",
    "41,-110,0,2.,0.\n",
    "41,-111,0,2.,0.\n",
    "41.5,-109,0,2.,0.\n",
    "41.5,-110,0,2.,0.\n",
    "41.5,-111,0,2.,0.\n",
    "```\n",
    "Notice that we've specified our choice of prior in line 4, where we've provided the path to the file containing our sampling functions, and lines 5 and onward specify the sensor locations. For more information on how the input file is constructed, see [Writing input files](inputs.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d153b74",
   "metadata": {},
   "source": [
    "#### Executing the example\n",
    "Once we've written an input file, the code can be run using any of the methods described in [Running the code](run-the-code). For example, to run interactively on a system with 16 cores per node, we could request 16 nodes:\n",
    "```shell\n",
    "salloc --nodes=32 --time=8:00:00\n",
    "```\n",
    "and then run the `eig_calc.py` script:\n",
    "```shell\n",
    "mpiexec --bind-to core --npernode 16 -n 512 python3 eig_calc.py uniform_inputs.dat uniform_outputs.npz 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85d3117",
   "metadata": {},
   "source": [
    "#### Visualizing outputs\n",
    "\n",
    "This code will produce outputs according to Section 3.3, and those outputs can be visualized in a variety of ways. \n",
    "For example, we could visualize the Expected Information Gain that the network would provide for all events in the domain. When using verbosity level `1`, the analysis code saves the KL divergence for all `4096 * 32` sampled datasets. We can load these data and then average across data realizations to compute the Information Gain for each sampled event. We can also load the latitude and longitude domain inside which the events were sampled and the sensor network that we analyzed:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "# Load outputs\n",
    "output_data = np.load('uniform_outputs.npz')\n",
    "\n",
    "# Get events\n",
    "thetas = output_data['theta_data']\n",
    "\n",
    "# Get ig values for each event\n",
    "kls = output_data['ig'].reshape((4096, 32))\n",
    "igs = kls.mean(axis=1)\n",
    "\n",
    "# Get theta domain\n",
    "lat_range = output_data['lat_range']\n",
    "lon_range = output_data['lon_range']\n",
    "\n",
    "# Get sensors, only store latitude and longitude\n",
    "sensors = output_data['sensors'][:,:2]\n",
    "```\n",
    "\n",
    "Then, we can fit a surrogate to the Information Gain values and predict the values at all the non-sampled points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15b25790",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Visualization helper functions\n",
    "def select_training_samples(samples, \n",
    "                            targets, \n",
    "                            depth_slice=0.5, \n",
    "                            mag_slice=0.5, \n",
    "                            depth_tol=5,\n",
    "                            mag_tol=.75,\n",
    "                            method='tol',\n",
    "                            verbose=0\n",
    "                           ):\n",
    "    if method=='tol':\n",
    "        depth_low = depth_slice - depth_tol\n",
    "        depth_high = depth_slice + depth_tol\n",
    "\n",
    "        mag_low = mag_slice - mag_tol\n",
    "        mag_high = mag_slice + mag_tol\n",
    "\n",
    "        # Mask that selects samples whose magnitude and depth are in the desired range\n",
    "        mask = [((samples[:,2]<=depth_high) & (samples[:,2]>=depth_low)) & \n",
    "                ((samples[:,3]<=mag_high) & (samples[:,3]>=mag_low))]\n",
    "\n",
    "        training_inputs = samples[tuple(mask)]\n",
    "        training_targets = targets[tuple(mask)]\n",
    "        if verbose==1:\n",
    "            print(f'Selected {training_inputs.shape[0]} samples to train with')\n",
    "        \n",
    "    elif method=='random':\n",
    "        idx = random.sample(range(0,len(samples)), 1000)\n",
    "        training_inputs = samples[idx]\n",
    "        training_targets = targets[idx]\n",
    "\n",
    "    return training_inputs, training_targets\n",
    "\n",
    "def plot_ig_surface(model, \n",
    "                 lat_range, \n",
    "                 lon_range,\n",
    "                 depth_slice=0.5,\n",
    "                 mag_slice=0.5,\n",
    "                 gridsize=50,\n",
    "                 plot_title=\"EIG plot\"\n",
    "                ):\n",
    "    # Create domain points to predict on\n",
    "    x = np.linspace(lat_range[0], lat_range[1], gridsize)\n",
    "    y = np.linspace(long_range[0], long_range[1], gridsize)\n",
    "    xv, yv = np.meshgrid(x, y)\n",
    "    xy = np.column_stack((xv.ravel(), yv.ravel())).T\n",
    "    \n",
    "    # Give each lat/lon domain point a depth and magnitude\n",
    "    domain = np.zeros((len(xy), 4))\n",
    "    domain[:, :2] = xy\n",
    "    domain[:, 2] = depth_slice\n",
    "    domain[:, 3] = mag_slice\n",
    "    \n",
    "    # Predict IG value at each domain point\n",
    "    preds = model.predict(domain)\n",
    "    \n",
    "    # Plot predictions with longitude for x and latitude for y\n",
    "    # Note that this is opposite of how the code processes them\n",
    "    im = plt.pcolormesh(yv, \n",
    "                        xv, \n",
    "                        preds.reshape((gridsize, gridsize)),\n",
    "                        shading=\"auto\",\n",
    "                        cmap=\"jet\"\n",
    "                       )\n",
    "    im.set_clim(preds.min(),preds.max()) # Standardize colorbar limits\n",
    "    \n",
    "    # Plot sensor network on map\n",
    "    plt.scatter(net[:, 1],net[:, 0],c=\"white\", label=\"sensors\")\n",
    "    \n",
    "    # Plot decorations\n",
    "    plt.title(plot_title)\n",
    "    ax.set_xlabel(\"Longitude\")\n",
    "    ax.set_ylabel(\"Latitude\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86b2c27",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor as GPR\n",
    "from skopt.learning.gaussian_process.kernels import RBF, WhiteKernel\n",
    "\n",
    "# Downsample training data since a GP will take a long time to train with 4096 points\n",
    "training_samples, training_targets = select_training_samples(thetas, igs)\n",
    "\n",
    "# Create and fit the model\n",
    "kernel = (1.0 * \n",
    "          RBF(length_scale=[1.0, 1.0,1.0,1.0], length_scale_bounds=(0.2, 1)) + \n",
    "          WhiteKernel(noise_level=0.1, noise_level_bounds=(1e-2, 5e-1))\n",
    "         )\n",
    "model = GPR(kernel=kernel,alpha=0.0, normalize_y=True)\n",
    "model.fit(training_samples, training_targets)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e831aa",
   "metadata": {},
   "source": [
    "Then we can make predictions on the whole domain and visualize the predictions:\n",
    "\n",
    "```python\n",
    "plot_ig_surface(model,\n",
    "                lat_range, \n",
    "                lon_range, \n",
    "                plot_title=\"Network analysis with uniform prior\"\n",
    "               )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956ca78f",
   "metadata": {},
   "source": [
    ":::{figure} ../figs/unif_analysis.png\n",
    "---\n",
    "width: 80 %\n",
    "---\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a7a094",
   "metadata": {},
   "source": [
    "### Analysis with nonuniform prior\n",
    "\n",
    "Instead of a uniform prior on seismic event locations, in this example we will assume depth is uniformly distributed and magnitude follows the same exponential distribution as in the previous example, but in this example we will assume location (latitude and longitude) follows a Gaussian mixture distribution as shown in this figure:\n",
    "\n",
    ":::{figure} ../figs/prior.png\n",
    "---\n",
    "width: 80 %\n",
    "---\n",
    ":::\n",
    "\n",
    "The functions that perform the sampling operations for $\\theta$ can be found in the file [`nonuniform_prior.py`](https://github.com/sandialabs/seismic_boed/blob/master/examples/sampling_files/nonuniform_prior.py).\n",
    "For more information on how to write a sampling file, see #TODO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d5a0ef",
   "metadata": {},
   "source": [
    "#### Inputs\n",
    "The input file we use for this example will be almost identical to the uniform prior example, with the only change being the line providing the path to the sampling file:\n",
    "```text\n",
    "4096\n",
    "16384\n",
    "32\n",
    "nonuniform_prior.py\n",
    "40.5,-109,0,2.,0.\n",
    "40.5,-110,0,2.,0.\n",
    "40.5,-111,0,2.,0.\n",
    "41,-109,0,2.,0.\n",
    "41,-110,0,2.,0.\n",
    "41,-111,0,2.,0.\n",
    "41.5,-109,0,2.,0.\n",
    "41.5,-110,0,2.,0.\n",
    "41.5,-111,0,2.,0.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8c6e06",
   "metadata": {},
   "source": [
    "#### Executing the example\n",
    "We will execute the network analysis the same way as in the uniform example, but we re-emphasize that any of the methods described in [Running the code](run-the-code) will work. On a system with 16 cores per node, we request 16 nodes:\n",
    "```shell\n",
    "salloc --nodes=32 --time=8:00:00\n",
    "```\n",
    "and then run the `eig_calc.py` script:\n",
    "```shell\n",
    "mpiexec --bind-to core --npernode 16 -n 512 python3 eig_calc.py nonuniform_inputs.dat nonuniform_outputs.npz 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22ee8b0",
   "metadata": {},
   "source": [
    "#### Visualizing the results\n",
    "We will visualize the results similarly to the previous example by creating a surface predicting the Expected Information Gain at each point in the lat/lon domain. We load the outputs of the analysis with the nonuniform prior:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "# Load outputs\n",
    "output_data = np.load('nonuniform_outputs.npz')\n",
    "\n",
    "# Get events\n",
    "thetas = output_data['theta_data']\n",
    "\n",
    "# Get ig values for each event\n",
    "kls = output_data['ig'].reshape((4096, 32))\n",
    "igs = kls.mean(axis=1)\n",
    "\n",
    "# Get theta domain\n",
    "lat_range = output_data['lat_range']\n",
    "lon_range = output_data['lon_range']\n",
    "\n",
    "# Get sensors, only store latitude and longitude\n",
    "sensors = output_data['sensors'][:,:2]\n",
    "```\n",
    "\n",
    "Next, we fit a surrogate to the Information Gain values and predict the values at all the non-sampled points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc499b6",
   "metadata": {},
   "source": [
    "```python\n",
    "# Downsample training data since a GP will take a long time to train with 4096 points\n",
    "training_samples, training_targets = select_training_samples(thetas, igs)\n",
    "\n",
    "# Create and fit the model\n",
    "kernel = (1.0 * \n",
    "          RBF(length_scale=[1.0, 1.0,1.0,1.0], length_scale_bounds=(0.2, 1)) + \n",
    "          WhiteKernel(noise_level=0.1, noise_level_bounds=(1e-2, 5e-1))\n",
    "         )\n",
    "model = GPR(kernel=kernel,alpha=0.0, normalize_y=True)\n",
    "model.fit(training_samples, training_targets)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7030bf44",
   "metadata": {},
   "source": [
    "Then we can make predictions on the whole domain and visualize the predictions:\n",
    "\n",
    "```python\n",
    "plot_ig_surface(model,\n",
    "                lat_range, \n",
    "                lon_range, \n",
    "                plot_title=\"Network analysis with nonuniform prior\"\n",
    "               )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62604087",
   "metadata": {},
   "source": [
    ":::{figure} ../figs/nonunif_analysis.png\n",
    "---\n",
    "width: 80 %\n",
    "---\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d403feba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}